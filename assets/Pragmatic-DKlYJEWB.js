import{j as e}from"./main-C57w9WFt.js";const a={title:"TDD, the fuzz, and pragmatics",date:"2024-09-10",slug:"pragmatics"};function s(n){const t={a:"a",em:"em",li:"li",ol:"ol",p:"p",ul:"ul",...n.components};return e.jsxs(e.Fragment,{children:[e.jsxs(t.p,{children:["I listened to an episode of the Future of Coding podcast last week, ",e.jsx(t.a,{href:"https://futureofcoding.org/episodes/073",children:"Moving Beyond Syntax: Lessons from 20 Years of Blocks Programming in AgentSheets by Alexander Repenning"}),"."]}),`
`,e.jsx(t.p,{children:"Had a thought."}),`
`,e.jsx(t.p,{children:'The podcast talked about syntax, semantics, and pragmatics, in which semantics means "understanding what a piece of code means" and pragmatics means "understanding what the code does / means in context of the system and code around it".'}),`
`,e.jsx(t.p,{children:`That podcast is frequently obsessed with visual programming, as are many of the covered papers. Their discussion was largely about blocks based programming and ways to visualize a block's potential input and output to play with it and better understand it in context. At least, that was my understanding. all good stuff.
I love the idea of visual programming, but that wasn't my takeaway. Rather, their discussion helped me frame the value of unit testing a bit differently.`}),`
`,e.jsxs(t.p,{children:["I'd argue that the main (or at least a major?) value prop of unit testing is as a tool for exploring the space of your code's ",e.jsx(t.em,{children:"pragmatics."})]}),`
`,e.jsx(t.p,{children:"Bear with me:"}),`
`,e.jsxs(t.ul,{children:[`
`,e.jsx(t.li,{children:"In an ideal visual testing system, you could see multiple inputs, or rather, incoming streams of information into your function. Maybe this would be literal function argument inputs. Maybe you're visually fiddling with the inputs to several blocks or nodes upstream of the logic you're currently tinkering with, in which case you're getting rapid feedback on the system's response to complex variations of input."}),`
`,e.jsx(t.li,{children:"In an ideal non-visual testing environment, your tests serve the same function."}),`
`]}),`
`,e.jsx(t.p,{children:`In text-based debugging tools, you can pause execution and inspect state. But even if you can adjust state on the fly mid-execution, you can't visualize several different permutations of state simultaneously.
With unit (or lightly integrated) tests, however, you'll frequently find yourself operating on a suite of tests that each run the code of interest with slightly different parameters.
In effect, you're fanning out across a range of possible inputs and states, and getting to "see" their effects on the code (nearly) simultaneously.`}),`
`,e.jsxs(t.p,{children:["This sort of fanning out of possibilities in rapid iterative development is magic. I don't know what other people's hypotheses are for ",e.jsx(t.em,{children:"why"}),` TDD is effective, but this really resonates with me.
I want to write a function and see that it produces expected results across a whole spectrum of possibilities at once.`]}),`
`,e.jsx(t.p,{children:"Follow-up thoughts:"}),`
`,e.jsxs(t.ol,{children:[`
`,e.jsx(t.li,{children:"I wonder how that same juxtaposition of a bunch of permutations could be made effective in more visual programming systems. Some kind of onion skin view? A literal fanning out, like a hand of cards, showing multiple inputs and their corresponding results? Text is dense, and a list of test cases fits in a console, but it's a bit harder to imagine in a nodes-and-wires paradigm."}),`
`,e.jsxs(t.li,{children:["I'd love to see a system where you write strongly typed functional code, but the input types go beyond string/int/object. I'd like to see each function include specifications like number ranges, RegExp for string values, and possibly more complex constraints in the relationship between multiple arguments. In addition, I imagining each publicly exported function or module should include a sample data generator designed to produce minimal, maximal, and randomized inputs that the function should be able to handle. This would be done to encourage fuzz testing, basically. Each component comes with its own defined ",e.jsx(t.em,{children:"range"}),' of acceptable parameters, which is used to help the programmer "visualize" the system and find the edges where the system fails.']}),`
`]})]})}function o(n={}){const{wrapper:t}=n.components||{};return t?e.jsx(t,{...n,children:e.jsx(s,{...n})}):s(n)}export{o as default,a as frontmatter};
